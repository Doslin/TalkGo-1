# 第 6阶段课程笔记

## 37 | 案例篇：DNS 解析时快时慢，我该怎么办？

DNS（Domain Name System），即域名系统，是互联网中最基础的一项服务，主要提供域名和 IP 地址之间映射关系的查询服务。

DNS 服务通过资源记录的方式，来管理所有数据，它支持 A、CNAME、MX、NS、PTR 等多种类型的记录。比如：A 记录，用来把域名转换成 IP 地址；CNAME 记录，用来创建别名；而 NS 记录，则表示该域名对应的域名服务器地址。



### **DNS查看或跟踪命令**

nslookup

dig trace功能



### **DNS相关配置**

在 /etc/resolv.conf 文件中，配置上 DNS 服务器就可以了



### **启用DNS缓存**

最简单的方法，就是使用 dnsmasq。启动方法

```
# /etc/init.d/dnsmasq start

- Starting DNS forwarder and DHCP server dnsmasq                    [ OK ]
```

修改 /etc/resolv.conf，将 DNS 服务器改为 dnsmasq 的监听地址，这儿是 127.0.0.1



### DNS优化方法

1. 对 DNS 解析的结果进行缓存。缓存是最有效的方法。
2. 对 DNS 解析的结果进行预取。这是浏览器等 Web 应用中最常用的方法，也就是说，不等用户点击页面上的超链接，浏览器就会在后台自动解析域名，并把结果缓存起来。
3. 使用 HTTPDNS 取代常规的 DNS 解析。这是很多移动应用会选择的方法，特别是如今域名劫持普遍存在，使用 HTTP 协议绕过链路中的 DNS 服务器，就可以避免域名劫持的问题。
4. 基于 DNS 的全局负载均衡（GSLB）。这不仅为服务提供了负载均衡和高可用的功能，还可以根据用户的位置，返回距离最近的 IP 地址。



## 38 | 案例篇：怎么使用 tcpdump 和 Wireshark 分析网络流量？

### tcpdump命令

tcpdump**使用选项**

![tcpdump使用选项](https://github.com/hwangyungping/TalkGo/blob/master/TalkGo读书会--第一期/PIC/06-01.png)

**tcpdump过滤表达式**

![tcpdump过滤表达式](https://github.com/hwangyungping/TalkGo/blob/master/TalkGo读书会--第一期/PIC/06-02.png)



### Wireshark工具

wireshark工作中常用，功能很强大，图形化界面方便查看，可以根据过滤条件分析，支持业务流程查看。

wireshark也支持命令行方式，很常见的场景是，tcpdump抓的长时间大包，比如1G文件共100个。可以通过命令行批量文件条件过滤后再分析。



## 39 | 案例篇：怎么缓解 DDoS 攻击带来的性能下降问题？

### DDoS是什么？

DDoS 的前身是 DoS（Denail of Service），即拒绝服务攻击，指利用大量的合理请求，来占用过多的目标资源，从而使目标服务无法响应正常请求。

DDoS（Distributed Denial of Service） 则是在 DoS 的基础上，采用了分布式架构，利用多台主机同时攻击目标主机。

DDoS 可以分为下面几种类型。

**第一种，耗尽带宽。**

**第二种，耗尽操作系统的资源。**

**第三种，消耗应用程序的运行资源。**



### DDoS 到底该怎么防御

**当 DDoS 报文到达服务器后，Linux 提供的机制只能缓解，而无法彻底解决。**

**需要应用程序考虑识别，并尽早拒绝掉这些恶意流量**，比如合理利用缓存、增加 WAF（Web Application Firewall）、使用 CDN 等等。



在 Linux 服务器中，你可以通过内核调优、DPDK、XDP 等多种方法，来增大服务器的抗攻击能力，降低 DDoS 对正常服务的影响。而在应用程序中，你可以利用各级缓存、 WAF、CDN 等方式，缓解 DDoS 对应用程序的影响。



## 40 | 案例篇：网络请求延迟变大了，我该怎么办？

**网络延迟查证方法:**

在发现网络延迟增大后，你可以用 traceroute、hping3、tcpdump、Wireshark、strace 等多种工具，来定位网络中的潜在问题。

比如，使用 hping3 以及 wrk 等工具，确认单次请求和并发请求情况的网络延迟是否正常。

使用 traceroute，确认路由是否正确，并查看路由中每一跳网关的延迟。

使用 tcpdump 和 Wireshark，确认网络包的收发是否正常。

使用 strace 等，观察应用程序对网络套接字的调用情况是否正常。

**这样，你就可以依次从路由、网络包的收发、再到应用程序等，逐层排查，直到定位问题根源。**



之前工作中对TCP场景下的网络时延做过分析，将相关知识点重新归纳下。补充在扩展知识点部分。



## 41 | 案例篇：如何优化 NAT 性能？（上）

### NAT的原理

NAT 的主要目的，是实现地址转换。根据实现方式的不同，NAT 可以分为三类：

1. 静态 NAT，即内网 IP 与公网 IP 是一对一的永久映射关系；
2. 动态 NAT，即内网 IP 从公网 IP 池中，动态选择一个进行映射；
3. 网络地址端口转换 NAPT（Network Address and Port Translation），即把内网 IP 映射到公网 IP 的不同端口上，让多个内网 IP 可以共享同一个公网 IP 地址。



NAPT 是目前最流行的 NAT 类型，我们在 Linux 中配置的 NAT 也是这种类型。

NAPT 分为三类。

第一类是源地址转换 SNAT，即目的地址不变，只替换源 IP 或源端口。

第二类是目的地址转换 DNAT，即源 IP 保持不变，只替换目的 IP 或者目的端口。D

第三类是双向地址转换，即同时使用 SNAT 和 DNAT。



**SNAT 和 DNAT 的过程图**

![SNAT 和 DNAT 的过程图](https://github.com/hwangyungping/TalkGo/blob/master/TalkGo读书会--第一期/PIC/06-03.png)



**网络数据包通过 Netfilter 时的工作流向****

![Netfilter工作流向](https://github.com/hwangyungping/TalkGo/blob/master/TalkGo读书会--第一期/PIC/06-04.png)



在这张图中，绿色背景的方框，表示表（table），用来管理链。L**inux 支持 4 种表，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。**跟 table 一起的白色背景方框，则表示链（chain），用来管理具体的 iptables 规则。每个表中可以包含多条链，比如：filter 表中，内置 INPUT、OUTPUT 和 FORWARD 链；nat 表中，内置 PREROUTING、POSTROUTING、OUTPUT 等。

**nat 表内置了三个链：**

PREROUTING，用于路由判断前所执行的规则，比如，对接收到的数据包进行 DNAT。

POSTROUTING，用于路由判断后所执行的规则，比如，对发送或转发的数据包进行 SNAT 或 MASQUERADE。OUTPUT，类似于 PREROUTING，但只处理从本机发送出去的包。



### SNAT配置方法

第一种方法，是为一个子网统一配置 SNAT，并由 Linux 选择默认的出口 IP。这实际上就是经常说的 MASQUERADE：

```
$ iptables -t nat -A POSTROUTING -s 192.168.0.0/16 -j MASQUERADE
```

第二种方法，是为具体的 IP 地址配置 SNAT，并指定转换后的源地址：

```
$ iptables -t nat -A POSTROUTING -s 192.168.0.2 -j SNAT --to-source 100.100.100.100
```



### DNAT配置方法

NAT 需要在 nat 表的 PREROUTING 或者 OUTPUT 链中配置，其中， PREROUTING 链更常用一些（因为它还可以用于转发的包）。

```
$ iptables -t nat -A PREROUTING -d 100.100.100.100 -j DNAT --to-destination 192.168.0.2
```



### 双向地址转换双向地址转换

同时添加 SNAT 和 DNAT 规则，为公网 IP 和内网 IP 实现一对一的映射关系，即：

```
$ iptables -t nat -A POSTROUTING -s 192.168.0.2 -j SNAT --to-source 100.100.100.100

$ iptables -t nat -A PREROUTING -d 100.100.100.100 -j DNAT --to-destination 192.168.0.2
```



在使用 iptables 配置 NAT 规则时，Linux 需要转发来自其他 IP 的网络包，所以你**千万不要忘记开启 Linux 的 IP 转发功能。**



## 42 | 案例篇：如何优化 NAT 性能？（下）

由于 NAT 基于 Linux 内核的连接跟踪机制来实现。所以，在分析 NAT 性能问题时，我们可以先从 conntrack 角度来分析，比如用 systemtap、perf 等，分析内核中 conntrack 的行文；然后，通过调整 netfilter 内核选项的参数，来进行优化。

Linux 这种通过连接跟踪机制实现的 NAT，也常被称为有状态的 NAT，而维护状态，也带来了很高的性能成本。所以，除了调整内核行为外，在不需要状态跟踪的场景下（比如只需要按预定的 IP 和端口进行映射，而不需要动态映射），**我们也可以使用无状态的 NAT （比如用 tc 或基于 DPDK 开发），来进一步提升性能**。





## 扩展知识点

### 网络时延

参考High Performance Browser Networking，网络时延如下：

*Let’s take a closer look at some common contributing components for a typical router on the Internet, which is responsible for relaying a message between the client and the server:*

**Propagation delay**

*Amount of time required for a message to travel from the sender to receiver, which is a function of distance over speed with which the signal propagates.*

**Transmission delay**

*Amount of time required to push all the packet’s bits into the link, which is a function of the packet’s length and data rate of the link.*

**Processing delay**

*Amount of time required to process the packet header, check for bit-level errors, and determine the packet’s destination.*

**Queuing delay**

*Amount of time the packet is waiting in the queue until it can be processed.*

*The total latency between the client and the server is the sum of all the delays just listed.*

也就是说，消息报文从客户端到服务端间发送时延，包括四部分：传输时延、发送时延、处理时延、排队时延。

其中传输时延与传输距离和传输媒介相关，这个通常无法改变。发送时延与链路的带宽相关，带宽越大发送越快，该时延与传输距离无关。

处理时延，主要是消息经由路由器交换机的转发处理时延。特别的，当途径路由器等网络设备处理能力受限时，则存在缓存排队时延。

在特定组网环境下，传输时延和发送时延是固定的。TCP协议栈存在处理时延和缓存时延，时延的优化从这两方面进行分析。



### **Linux下TCP/IP内核参数优化**

#### **/proc/sys/net目录**

所有的TCP/IP参数都位于/proc/sys/net目录下（请注意，对/proc/sys/net目录下内容的修改都是临时的，任何修改在系统重启后都会丢失），例如下面这些重要的参数：

 

| **参数（路径**+**文件）**               | **描述**                                                     | **默认值**            | **优化值**             |
| --------------------------------------- | ------------------------------------------------------------ | --------------------- | ---------------------- |
| /proc/sys/net/core/rmem_default         | 默认的TCP数据接收窗口大小（字节）。                          | 229376                | 256960                 |
| /proc/sys/net/core/rmem_max             | 最大的TCP数据接收窗口（字节）。                              | 131071                | 513920                 |
| /proc/sys/net/core/wmem_default         | 默认的TCP数据发送窗口大小（字节）。                          | 229376                | 256960                 |
| /proc/sys/net/core/wmem_max             | 最大的TCP数据发送窗口（字节）。                              | 131071                | 513920                 |
| /proc/sys/net/core/netdev_max_backlog   | 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。 | 1000                  | 2000                   |
| **/proc/sys/net/core/somaxconn**        | 定义了系统中每一个端口最大的监听队列的长度，这是个全局的参数。 | 128                   | 2048                   |
| /proc/sys/net/core/optmem_max           | 表示每个套接字所允许的最大缓冲区的大小。                     | 20480                 | 81920                  |
| /proc/sys/net/ipv4/tcp_mem              | 确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。第一个值是内存使用的下限；第二个值是内存压力模式开始对缓冲区使用应用压力的上限；第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。对于较大的BDP可以增大这些值（注意，其单位是内存页而不是字节）。 | 94011  125351  188022 | 131072  262144  524288 |
| **/proc/sys/net/ipv4/tcp_rmem**         | 为自动调优定义socket使用的内存。第一个值是为socket接收缓冲区分配的最少字节数；第二个值是默认值（该值会被rmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是接收缓冲区空间的最大字节数（该值会被rmem_max覆盖）。 | 4096  87380  4011232  | 8760  256960  4088000  |
| **/proc/sys/net/ipv4/tcp_wmem**         | 为自动调优定义socket使用的内存。第一个值是为socket发送缓冲区分配的最少字节数；第二个值是默认值（该值会被wmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是发送缓冲区空间的最大字节数（该值会被wmem_max覆盖）。 | 4096  16384  4011232  | 8760  256960  4088000  |
| /proc/sys/net/ipv4/tcp_keepalive_time   | TCP发送keepalive探测消息的间隔时间（秒），用于确认TCP连接是否有效。 | 7200                  | 1800                   |
| /proc/sys/net/ipv4/tcp_keepalive_intvl  | 探测消息未获得响应时，重发该消息的间隔时间（秒）。           | 75                    | 30                     |
| /proc/sys/net/ipv4/tcp_keepalive_probes | 在认定TCP连接失效之前，最多发送多少个keepalive探测消息。     | 9                     | 3                      |
| /proc/sys/net/ipv4/tcp_sack             | 启用有选择的应答（1表示启用），通过有选择地应答乱序接收到的报文来提高性能，让发送者只发送丢失的报文段，（对于广域网通信来说）这个选项应该启用，但是会增加对CPU的占用。 | 1                     | 1                      |
| /proc/sys/net/ipv4/tcp_fack             | 启用转发应答，可以进行有选择应答（SACK）从而减少拥塞情况的发生，这个选项也应该启用。 | 1                     | 1                      |
| /proc/sys/net/ipv4/tcp_timestamps       | TCP时间戳（会在TCP包头增加12个字节），以一种比重发超时更精确的方法（参考RFC 1323）来启用对RTT 的计算，为实现更好的性能应该启用这个选项。 | 1                     | 1                      |
| /proc/sys/net/ipv4/tcp_window_scaling   | 启用RFC 1323定义的window scaling，要支持超过64KB的TCP窗口，必须启用该值（1表示启用），TCP窗口最大至1GB，TCP连接双方都启用时才生效。 | 1                     | 1                      |
| **/proc/sys/net/ipv4/tcp_syncookies**   | 表示是否打开TCP同步标签（syncookie），内核必须打开了CONFIG_SYN_COOKIES项进行编译，同步标签可以防止一个套接字在有过多试图连接到达时引起过载。 | 1                     | 1                      |
| /proc/sys/net/ipv4/tcp_tw_reuse         | 表示是否允许将处于TIME-WAIT状态的socket（TIME-WAIT的端口）用于新的TCP连接 。 | 0                     | 1                      |
| /proc/sys/net/ipv4/tcp_tw_recycle       | 能够更快地回收TIME-WAIT套接字。                              | 0                     | 1                      |
| /proc/sys/net/ipv4/tcp_fin_timeout      | 对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间（秒）。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。 | 60                    | 30                     |
| /proc/sys/net/ipv4/ip_local_port_range  | 表示TCP/UDP协议允许使用的本地端口号                          | 32768  61000          | 1024  65000            |
| /proc/sys/net/ipv4/tcp_max_syn_backlog  | 对于还未获得对方确认的连接请求，可保存在队列中的最大数目。如果服务器经常出现过载，可以尝试增加这个数字。 | 2048                  | 2048                   |
| **/proc/sys/net/ipv4/tcp_low_latency**  | **允许TCP/IP栈适应在高吞吐量情况下低延时的情况，这个选项应该禁用。** | 0                     | 1                      |
| /proc/sys/net/ipv4/tcp_westwood         | 启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化，对于WAN 通信来说应该启用这个选项。 | 0                     |                        |
| /proc/sys/net/ipv4/tcp_bic              | 为快速长距离网络启用Binary Increase Congestion，这样可以更好地利用以GB速度进行操作的链接，对于WAN通信应该启用这个选项。 | 1                     |                        |

 

说明：

如上相关TCP参数需要逐个各个分析，调整的参数值也需要分析，对于CNS和云平台环境需要根据具体系统配置进行调整，上面给出的优化值是网络基于HTTP服务器给出的参考值。

 

#### **/etc/sysctl.conf文件**

/etc/sysctl.conf是一个允许你改变正在运行中的Linux系统的接口。它包含一些TCP/IP堆栈和虚拟内存系统的高级选项，可用来控制Linux网络配置，由于/proc/sys/net目录内容的临时性，建议把TCPIP参数的修改添加到/etc/sysctl.conf文件, 然后保存文件，使用命令“/sbin/sysctl –p”使之立即生效。具体修改方案参照上文：

 

net.core.rmem_default = 256960

net.core.rmem_max = 513920

net.core.wmem_default = 256960

net.core.wmem_max = 513920

net.core.netdev_max_backlog = 2000

net.core.somaxconn = 2048

net.core.optmem_max = 81920

net.ipv4.tcp_mem = 131072  262144  524288

net.ipv4.tcp_rmem = 8760  256960  4088000

net.ipv4.tcp_wmem = 8760  256960  4088000

net.ipv4.tcp_keepalive_time = 1800

net.ipv4.tcp_keepalive_intvl = 30

net.ipv4.tcp_keepalive_probes = 3

net.ipv4.tcp_sack = 1

net.ipv4.tcp_fack = 1

net.ipv4.tcp_timestamps = 1

net.ipv4.tcp_window_scaling = 1

net.ipv4.tcp_syncookies = 1

net.ipv4.tcp_tw_reuse = 1

net.ipv4.tcp_tw_recycle = 1

net.ipv4.tcp_fin_timeout = 30

net.ipv4.ip_local_port_range = 1024  65000

net.ipv4.tcp_max_syn_backlog = 2048

#### **tcp_low_latency选项**

特别对tcp_low_latency进行关注说明，根据配置选项的名称可以性能效率与低时延做优先级。当tcp_low_latency设为0时，以整个操作系统的效率优先，此时TCP会通过使用prequeue队列，使网络软中断的执行时间缩短，回ACK的时机延后，进程读取TCP套接字时略延后。



### **Nagle算法**

TCP为了达到最好的性能，使用尽可能多的可用数据来填充每个报文。当没有足够的数据来填充 payload 时（也称为最大报文段长度（maximum segment size） 或 MSS），TCP 就会采用 Nagle 算法自动将一些小的缓冲区连接到一个报文段中。这样可以通过最小化所发送的报文的数量来提高应用程序的效率，并减轻整体的网络拥塞问题。**Nagle算法的基本定义是任意时刻，最多只能有一个未被确认的小段。** 所谓“小段”，指的是小于MSS尺寸的数据块，所谓“未被确认”，是指一个数据块发送出去后，没有收到对方发送的ACK确认该数据已收到。

Nagle算法的规则：

（1）如果包长度达到MSS，则允许发送；

（2）如果该包含有FIN，则允许发送；

（3）设置了TCP_NODELAY选项，则允许发送；

（4）未设置TCP_CORK选项时，若所有发出去的小数据包（包长度小于MSS）均被确认，则允许发送；

（5）上述条件都未满足，但发生了超时（一般为200ms），则立即发送。

关于Nagle算法的细节，网络有很多帖子描述，这里就不摘录了。

Nagle算法提高了网络利用效率，但同时也引入了时延，其对于成块数据传输场景是合适的，但对于交互数据流场景，特别是IOT应用时间敏感场景下，由此带来的时延给应用带来影响。特别的，当发送端启用Nagle算法，而接收端启用延时确认ACK时，会带来40ms的时延。

 

### 延时确认**ACK**

关于延时确认ACK，其目的意义如其Wiki主页所述：

*In essence, several ACK responses may be combined together into a single response, reducing protocol overhead.*

在TCP双向数据传输是一来一回的场景下，Delay ACK可以省去所有的纯ACK段的发送。典型的比如远程终端登录(需要回显的如telnet，ssh之类)。Linux为这种双向数据传输取了个名字，叫做pingpong。Linux实现的其自适应Delay ACK换句话说就是Linux的协议栈可以自动识别当前是否是pingpong(即RW场景或者说完全交互场景)场景，从而依照这个判断来动态开启或者关闭Delay ACK。TCP之所以使能Delay ACK，其实并不是真的想ACK被Delay，而是期望数据的发送可以把ACK捎带过去，如果没有数据可发送，在ATO时间过后，ACK还是要发送的，毕竟ACK就是TCP的时钟。

虽然Linux提供了通过TCP_QUICKACK选项来关闭延时确认（也就是启动即时确认），man tcp查看帮助说明如下：

*TCP_QUICKACK (since Linux 2.4.4)*

*Enable  quickack  mode  if  set or disable quickack mode if cleared.  In quickack mode, acks are sent immediately, rather than delayed if needed in accordance to normal TCP operation.  This flag is not permanent, it only enables a switch to or from quickack mode.  Subsequent operation  of  the  TCP  protocol will once again enter/leave quickack mode depending on internal protocol processing and factors such as delayed ack timeouts occurring and data transfer.  This option should not be used in code intended to be portable.*

特别的，该配置并不永久生效只生效一次，每次调用recv后需要重新设置。也可以认为，是否延时确认ACK并非想象那么由一个开关控制，其涉及到TCP协议栈内部的一些细节逻辑，这里不做过多讨论。可以参考网络帖子**[10]**。

  

### 拥塞窗口与慢启动

滑动窗口协议是传输层进行流控的一种措施，接收方通过通告发送方自己的窗口大小，从而控制发送方的发送速度，从而达到防止发送方发送速度过快而导致自己被淹没的目的。虽然流量控制可以避免发送方过载接收方，但是却无法避免过载网络，这是因为接收窗口「rwnd」只反映了服务器个体的情况，却无法反映网络整体的情况。为了避免过载网络的问题，慢启动引入了拥塞窗口「cwnd」的概念，用来表示发送方在得到接收方确认前，最大允许传输的未经确认的数据。

通常接收窗口「rwnd」的合理值取决于BDP的大小，也就是带宽和延迟的乘积。假设带宽是 100Mbps，延迟是 100ms，那么计算过程如下：

*BDP = 100Mbps \* 100ms = (100 / 8) \* (100 / 1000) = 1.25MB*

Linux中通过配置内核参数里接收缓冲的大小，进而可以控制接收窗口的大小：

*shell> sysctl -a | grep mem*

*net.ipv4.tcp_rmem = <MIN> <DEFAULT> <MAX>*

Linux本身有一个缓冲大小自动调优的机制，窗口的实际大小会自动在最小值和最大值之间浮动，以期找到性能和资源的平衡点。通过如下方式可以确认缓冲大小自动调优机制的状态（0：关闭、1：开启）：

*shell> sysctl -a | grep tcp_moderate_rcvbuf*

如果缓冲大小自动调优机制是关闭状态，那么就把缓冲的缺省值设置为BDP；如果缓冲大小自动调优机制是开启状态，那么就把缓冲的最大值设置为BDP。

一般来说「cwnd」的初始值取决于MSS的大小，计算方法如下：

*min(4 \* MSS, max(2 \* MSS, 4380))*

以太网标准的MSS大小通常是1460，所以「cwnd」的初始值是3MSS。加大「cwnd」初始值可以提升数据传输效率，可以参考资料**[15]**。

Google在这方面做了大量的研究，权衡了效率和稳定性之后，最终给出的建议是10MSS。



### 网络时延优化

### 关闭Nagle算法

对于小数据包场景下，发送端关闭Nagle算法减少时延。据知Nginx中也是默认关闭Nagle算法。Wiki中也提供了相应的方法：

If the application is transmitting data in smaller chunks and expecting periodic acknowledgment replies, this negative interaction can occur. To prevent this delay, the application layer needs to continuously send data without waiting for acknowledgment replies. Alternatively, Nagle's algorithm may be disabled by the application on the sending side.

禁用Nagle 算法比较简单，通过设置 TCP_NODELAY socket 选项即可，man tcp有如下解释：

TCP_NODELAY

If  set, disable the Nagle algorithm.  This means that segments are always sent as soon as possible, even if there is only a small amount of data.  When not set, data is buffered until there is a sufficient amount  to  send  out,thereby  avoiding  the  frequent sending of small packets, **which results in poor utilization of the network**.  

 

### 延时确认Delay ACK

关于延时确认Delay ACK，通过现网抓包当时分析发现似乎并不一致，甚至一度怀疑loopback接口下Delay ACK关闭，网络上甚至找到OpenBSD change log：“Do not delay ACKs on connections using loopback interfaces.”。直到搜索到网络帖子**[10]**对延时确认的机制原理才释疑。不过，确实有一些针对loopback接口做了时延优化和效率提升，比如Windows Server。

考虑到Delay ACK只有与Nagle算法一起配合才会出现200ms时延，且设置Delay ACK仅生效一次，因此在关闭Nalge算法下，延时确认Delay ACK不关闭，毕竟有它的存在的意义。

 

### 拥塞窗口优化

关闭慢启动

shell> sysctl net.ipv4.tcp_slow_start_after_idle

shell> sysctl -w net.ipv4.tcp_slow_start_after_idle=0

 

调整「cwnd」初始值为10 MSS

shell> ip route change default via 192.168.1.1 dev eth0  proto static **initcwnd 10**

### 打开tcp_low_latency

shell> sysctl -a | grep tcp_low_latency

net.ipv4.tcp_low_latency = 0

默认值为0，为了提升TCP的处理效率关闭该选项。若打开该选项，可以提升TCP的处理时延。这点需要实际验证下，是否有多大的时延提升，若小于1ms级别，则意义不大。因为毕竟性能和效率是需要平衡的。

shell> sysctl -w net.ipv4.tcp_low_latency=1